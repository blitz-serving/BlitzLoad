diff --git a/offline_infer_vllm.py b/offline_infer_vllm.py
new file mode 100644
index 000000000..b16fd85f8
--- /dev/null
+++ b/offline_infer_vllm.py
@@ -0,0 +1,17 @@
+from vllm import LLM, SamplingParams
+
+model_path = "/nvme/ly/models/Qwen2.5-7B"
+prompts = ["haha how are you"]
+sampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=10)
+
+llm = LLM(model=model_path, enforce_eager=True, max_model_len=4096)
+
+outputs = llm.generate(prompts, sampling_params)
+
+for output in outputs:
+    prompt = output.prompt
+    generated_text = output.outputs[0].text
+    tokens = output.outputs[0].token_ids
+    print(
+        f"Prompt: {prompt!r}, Generated text: {generated_text!r}, output tokens: {tokens}"
+    )
diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
index 915f14a29..50dc79246 100644
--- a/vllm/entrypoints/llm.py
+++ b/vllm/entrypoints/llm.py
@@ -2,6 +2,8 @@
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
 import itertools
+import blitz_lib
+
 from collections.abc import Sequence
 from contextlib import contextmanager
 from typing import (TYPE_CHECKING, Any, Callable, ClassVar, Optional, Union,
@@ -276,7 +278,7 @@ class LLM:
         )
 
         log_non_default_args(engine_args)
-
+        blitz_lib.pull_model(model, tensor_parallel_size * 1, tensor_parallel_size, 1)
         # Create the Engine (autoselects V0 vs V1)
         self.llm_engine = LLMEngine.from_engine_args(
             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)
diff --git a/vllm/model_executor/layers/linear.py b/vllm/model_executor/layers/linear.py
index 75391c51f..212bb6134 100644
--- a/vllm/model_executor/layers/linear.py
+++ b/vllm/model_executor/layers/linear.py
@@ -2,6 +2,7 @@
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
 import itertools
+from blitz_lib import vllm_hook
 from abc import abstractmethod
 from typing import Any, Literal, Optional, Union
 
@@ -333,6 +334,7 @@ class ReplicatedLinear(LinearBase):
         else:
             self.register_parameter("bias", None)
 
+    @vllm_hook
     def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
         # If the weight on disk does not have a shape, give it one
         # (such scales for AutoFp8).
@@ -528,6 +530,7 @@ class ColumnParallelLinear(LinearBase):
 
         self.tp_rank = get_tensor_model_parallel_rank()
 
+    @vllm_hook
     def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
 
         output_dim = getattr(param, "output_dim", None)
@@ -656,6 +659,7 @@ class MergedColumnParallelLinear(ColumnParallelLinear):
                          prefix=prefix,
                          return_bias=return_bias)
 
+    @vllm_hook
     def weight_loader(self,
                       param: Parameter,
                       loaded_weight: torch.Tensor,
@@ -1045,6 +1049,7 @@ class QKVParallelLinear(ColumnParallelLinear):
                               shard_offset=shard_offset,
                               shard_size=shard_size)
 
+    @vllm_hook
     def weight_loader(self,
                       param: Parameter,
                       loaded_weight: torch.Tensor,
@@ -1317,6 +1322,7 @@ class RowParallelLinear(LinearBase):
         else:
             self.register_parameter("bias", None)
 
+    @vllm_hook
     def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
         input_dim = getattr(param, "input_dim", None)
         use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
diff --git a/vllm/model_executor/layers/vocab_parallel_embedding.py b/vllm/model_executor/layers/vocab_parallel_embedding.py
index 9f223998e..e10f3ef16 100644
--- a/vllm/model_executor/layers/vocab_parallel_embedding.py
+++ b/vllm/model_executor/layers/vocab_parallel_embedding.py
@@ -210,7 +210,9 @@ class VocabParallelEmbedding(CustomOp):
                  prefix: str = ""):
         super().__init__()
 
+        self.prefix = prefix
         # Keep the input dimensions.
+        self.prefix = prefix
         tp_rank = get_tensor_model_parallel_rank()
         self.tp_size = get_tensor_model_parallel_world_size()
         self.num_embeddings = num_embeddings
@@ -368,8 +370,9 @@ class VocabParallelEmbedding(CustomOp):
         # If parameter does not have output dim, then it should
         # be copied onto all gpus (e.g. g_idx for act_order gptq).
         if output_dim is None:
-            assert param.data.shape == loaded_weight.shape
-            param.data.copy_(loaded_weight)
+            import blitz_lib
+            print("Loading tensor Embedding")
+            blitz_lib.load_tensor(param, self.prefix)
             return
 
         # Shard indexes for loading the weight
@@ -386,12 +389,16 @@ class VocabParallelEmbedding(CustomOp):
             start_idx = start_idx // packed_factor
             shard_size = shard_size // packed_factor
         else:
-            assert loaded_weight.shape[output_dim] == self.org_vocab_size
+            pass
+            # assert loaded_weight.shape[output_dim] == self.org_vocab_size
 
         # Copy the data. Select chunk corresponding to current shard.
-        loaded_weight = loaded_weight.narrow(output_dim, start_idx, shard_size)
-        param[:loaded_weight.shape[0]].data.copy_(loaded_weight)
-        param[loaded_weight.shape[0]:].data.fill_(0)
+        import blitz_lib
+        print("Tensor embedding")
+        blitz_lib.load_tensor(param, self.prefix)
+        # loaded_weight = loaded_weight.narrow(output_dim, start_idx, shard_size)
+        # param[:loaded_weight.shape[0]].data.copy_(loaded_weight)
+        # param[loaded_weight.shape[0]:].data.fill_(0)
 
     def forward(self, input_):
         if self.tp_size > 1:
diff --git a/vllm/model_executor/model_loader/default_loader.py b/vllm/model_executor/model_loader/default_loader.py
index 2b8e44275..69898376a 100644
--- a/vllm/model_executor/model_loader/default_loader.py
+++ b/vllm/model_executor/model_loader/default_loader.py
@@ -20,7 +20,7 @@ from vllm.model_executor.model_loader.weight_utils import (
     download_safetensors_index_file_from_hf, download_weights_from_hf,
     fastsafetensors_weights_iterator, filter_duplicate_safetensors_files,
     filter_files_not_needed_for_inference, get_lock, np_cache_weights_iterator,
-    pt_weights_iterator, safetensors_weights_iterator)
+    pt_weights_iterator, safetensors_weights_iterator, dangertensors_weights_iterator)
 from vllm.platforms import current_platform
 
 logger = init_logger(__name__)
@@ -171,13 +171,36 @@ class DefaultModelLoader(BaseModelLoader):
 
         return hf_folder, hf_weights_files, use_safetensors
 
+    def _prepare_dangertensors(self, directory):
+        import re
+        pattern = re.compile(r"dangertensors\.(\d+)\.meta$")
+        files_with_number = []
+        import blitz_lib
+        rank = blitz_lib.get_rank()
+        file_name = f"dangertensors.{rank}.meta"
+
+        for root, _, files in os.walk(directory):
+            for f in files:
+                if f == file_name:
+                    files_with_number.append((rank, os.path.join(root, f)))
+                # match = pattern.match(f)
+                # if match:
+                #     number = int(match.group(1))
+                #     files_with_number.append((number, os.path.join(root, f)))
+
+        # files_with_number.sort(key=lambda x: x[0])
+        logger.info(f"Get Meta files: {files_with_number}")
+        return [path for _, path in files_with_number]
+
     def _get_weights_iterator(
             self, source: "Source"
     ) -> Generator[tuple[str, torch.Tensor], None, None]:
         """Get an iterator for the model weights based on the load format."""
-        hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
-            source.model_or_path, source.revision, source.fall_back_to_pt,
-            source.allow_patterns_overrides)
+        # hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
+        #     source.model_or_path, source.revision, source.fall_back_to_pt,
+        #     source.allow_patterns_overrides)
+        hf_folder, hf_weights_files, use_safetensors, use_dangetensors = ("", [], False, True)
+        danger_metas = self._prepare_dangertensors(source.model_or_path)
         if self.load_config.load_format == "npcache":
             # Currently np_cache only support *.bin checkpoints
             assert use_safetensors is False
@@ -199,6 +222,12 @@ class DefaultModelLoader(BaseModelLoader):
                     hf_weights_files,
                     self.load_config.use_tqdm_on_load,
                 )
+        elif use_dangetensors:
+            import blitz_lib
+            pull_done = False
+            while not pull_done:
+                pull_done = blitz_lib.check_model(source.model_or_path)
+            weights_iterator = dangertensors_weights_iterator(danger_metas, self.load_config.use_tqdm_on_load)
         else:
             weights_iterator = pt_weights_iterator(
                 hf_weights_files,
@@ -221,7 +250,9 @@ class DefaultModelLoader(BaseModelLoader):
         if self.counter_before_loading_weights == 0.0:
             self.counter_before_loading_weights = time.perf_counter()
         # Apply the prefix.
-        return ((source.prefix + name, tensor)
+        # return ((source.prefix + name, tensor)
+        #         for (name, tensor) in weights_iterator)
+        return ((name, tensor)
                 for (name, tensor) in weights_iterator)
 
     def get_all_weights(
@@ -259,6 +290,8 @@ class DefaultModelLoader(BaseModelLoader):
         loaded_weights = model.load_weights(
             self.get_all_weights(model_config, model))
         self.counter_after_loading_weights = time.perf_counter()
+        import blitz_lib
+        blitz_lib.reset_status()
         logger.info(
             "Loading weights took %.2f seconds",
             self.counter_after_loading_weights -
diff --git a/vllm/model_executor/model_loader/weight_utils.py b/vllm/model_executor/model_loader/weight_utils.py
index 78b186265..1207ea47b 100644
--- a/vllm/model_executor/model_loader/weight_utils.py
+++ b/vllm/model_executor/model_loader/weight_utils.py
@@ -29,6 +29,10 @@ from vllm.model_executor.layers.quantization import (QuantizationConfig,
 from vllm.platforms import current_platform
 from vllm.utils import PlaceholderModule
 
+from vllm.logger import init_logger
+
+logger = init_logger(__name__)
+
 try:
     from runai_model_streamer import SafetensorsStreamer
 except (ImportError, OSError):
@@ -60,6 +64,7 @@ logger = init_logger(__name__)
 # system reboots, so users will not complain about annoying lock files
 temp_dir = tempfile.gettempdir()
 
+alltime = 0
 
 def enable_hf_transfer():
     """automatically activates hf_transfer
@@ -476,6 +481,31 @@ def safetensors_weights_iterator(
                 yield name, param
 
 
+def dangertensors_weights_iterator(
+    danger_metas: list[str],
+    use_tqdm_on_load: bool,
+) -> Generator[tuple[str, torch.Tensor], None, None]:
+    """Iterate over the weights in the model safetensor files."""
+    for st_file in tqdm(
+        danger_metas,
+        desc="Loading safetensors checkpoint shards",
+        disable=not enable_tqdm(use_tqdm_on_load),
+        bar_format=_BAR_FORMAT,
+    ):
+        with open(st_file, 'r', encoding='utf-8') as f:
+            lines = f.readlines()[1:]
+            for line in lines:
+                line = line.strip()
+                name, _ = line.rsplit(" ", 1)
+                yield name, torch.zeros(1)
+        pass
+        # with safe_open(st_file, framework="pt") as f:
+        #     for name in f.keys():  # noqa: SIM118
+        #         # param = f.get_tensor(name)
+        #         param = torch.zeros(1)
+        #         yield name, param
+                # yield name, param
+
 def runai_safetensors_weights_iterator(
     hf_weights_files: list[str],
     use_tqdm_on_load: bool,
@@ -611,21 +641,37 @@ def convert_pyslice_to_tensor(x: Any) -> torch.Tensor:
     return x
 
 
-def default_weight_loader(param: torch.Tensor,
-                          loaded_weight: torch.Tensor) -> None:
-    """Default weight loader."""
+# def default_weight_loader(param: torch.Tensor,
+#                           loaded_weight: torch.Tensor) -> None:
+#     """Default weight loader."""
+#     try:
+#         if param.numel() == 1 and loaded_weight.numel() == 1:
+#             # Sometimes scalar values aren't considered tensors with shapes
+#             # so if both param and loaded_weight are a scalar,
+#             # "broadcast" instead of copy
+#             param.data.fill_(loaded_weight.item())
+#         else:
+#             assert param.size() == loaded_weight.size(), (
+#                 f"Attempted to load weight ({loaded_weight.size()}) "
+#                 f"into parameter ({param.size()})")
+
+#             param.data.copy_(loaded_weight)
+#     except Exception:
+#         # NOTE: This exception is added for the purpose of setting breakpoint to
+#         # debug weight loading issues.
+#         raise
+
+
+def default_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor, name = None) -> None:
+    """Blitz weight loader."""
     try:
-        if param.numel() == 1 and loaded_weight.numel() == 1:
-            # Sometimes scalar values aren't considered tensors with shapes
-            # so if both param and loaded_weight are a scalar,
-            # "broadcast" instead of copy
-            param.data.fill_(loaded_weight.item())
+        import blitz_lib 
+        # if name:
+        #     print(name, param.device, param.dtype, param.shape)
+        if name: 
+            blitz_lib.load_tensor(param, name)
         else:
-            assert param.size() == loaded_weight.size(), (
-                f"Attempted to load weight ({loaded_weight.size()}) "
-                f"into parameter ({param.size()})")
-
-            param.data.copy_(loaded_weight)
+            blitz_lib.load_tensor(param, "")
     except Exception:
         # NOTE: This exception is added for the purpose of setting breakpoint to
         # debug weight loading issues.
diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py
index 24cd448d8..b4f8152f1 100644
--- a/vllm/model_executor/models/llama.py
+++ b/vllm/model_executor/models/llama.py
@@ -56,6 +56,11 @@ from .utils import (AutoWeightsLoader, PPMissingLayer, extract_layer_index,
                     maybe_prefix)
 
 
+from vllm.logger import init_logger
+logger = init_logger(__name__)
+
+cpu2gpu_time = 0
+
 class LlamaMLP(nn.Module):
 
     def __init__(
@@ -398,6 +403,8 @@ class LlamaModel(nn.Module):
 
     def load_weights(self, weights: Iterable[tuple[str,
                                                    torch.Tensor]]) -> set[str]:
+        global cpu2gpu_time
+        import time
         stacked_params_mapping = [
             # (param_name, shard_name, shard_id)
             (".qkv_proj", ".q_proj", "q"),
@@ -409,6 +416,7 @@ class LlamaModel(nn.Module):
         params_dict = dict(self.named_parameters())
         loaded_params: set[str] = set()
         for name, loaded_weight in weights:
+            logger.info(f"Loaded weight device: {loaded_weight.device}")
             if "rotary_emb.inv_freq" in name:
                 continue
             if ("rotary_emb.cos_cached" in name
@@ -424,7 +432,13 @@ class LlamaModel(nn.Module):
                                         default_weight_loader)
                 loaded_weight = (loaded_weight if loaded_weight.dim() == 0 else
                                  loaded_weight[0])
+                start = time.time()
                 weight_loader(param, loaded_weight)
+                end = time.time()
+                cpu2gpu_time += end - start
+                logger.info(
+                    f"weight loader: {weight_loader},CPU to GPU0 accumulated time: {cpu2gpu_time}"
+                )
                 loaded_params.add(scale_name)
                 continue
             if "scale" in name:
@@ -444,8 +458,15 @@ class LlamaModel(nn.Module):
                     continue
 
                 param = params_dict[name]
+                logger.info(f"Param shape: {param.shape}, param device: {param.device}")
                 weight_loader = param.weight_loader
+                start = time.time()
                 weight_loader(param, loaded_weight, shard_id)
+                end = time.time()
+                cpu2gpu_time += end - start
+                logger.info(
+                    f"weight loader: {weight_loader}, param device: {param.device}, CPU to GPU1 accumulated time: {cpu2gpu_time}"
+                )
                 break
             else:
                 # Skip loading extra bias for GPTQ models.
@@ -456,9 +477,14 @@ class LlamaModel(nn.Module):
                     continue
 
                 param = params_dict[name]
-                weight_loader = getattr(param, "weight_loader",
-                                        default_weight_loader)
+                weight_loader = getattr(param, "weight_loader", default_weight_loader)
+                start = time.time()
                 weight_loader(param, loaded_weight)
+                end = time.time()
+                cpu2gpu_time += end - start
+                logger.info(
+                    f"weight loader: {weight_loader},CPU to GPU2 accumulated time: {cpu2gpu_time}"
+                )
             loaded_params.add(name)
         return loaded_params
 
diff --git a/vllm/model_executor/models/qwen2.py b/vllm/model_executor/models/qwen2.py
index 7304fbf12..52513f29c 100644
--- a/vllm/model_executor/models/qwen2.py
+++ b/vllm/model_executor/models/qwen2.py
@@ -396,7 +396,10 @@ class Qwen2Model(nn.Module):
                                         default_weight_loader)
                 loaded_weight = (loaded_weight if loaded_weight.dim() == 0 else
                                  loaded_weight[0])
-                weight_loader(param, loaded_weight)
+                if weight_loader == default_weight_loader:
+                    weight_loader(param, loaded_weight, name)
+                else:
+                    weight_loader(param, loaded_weight)
                 loaded_params.add(scale_name)
                 continue
             for (param_name, weight_name, shard_id) in stacked_params_mapping:
@@ -417,7 +420,7 @@ class Qwen2Model(nn.Module):
                 weight_loader = getattr(param, "weight_loader",
                                         default_weight_loader)
                 if weight_loader == default_weight_loader:
-                    weight_loader(param, loaded_weight)
+                    weight_loader(param, loaded_weight, name)
                 else:
                     weight_loader(param, loaded_weight, shard_id)
                 break
@@ -434,7 +437,10 @@ class Qwen2Model(nn.Module):
                 param = params_dict[name]
                 weight_loader = getattr(param, "weight_loader",
                                         default_weight_loader)
-                weight_loader(param, loaded_weight)
+                if weight_loader == default_weight_loader:
+                    weight_loader(param, loaded_weight, name)
+                else:
+                    weight_loader(param, loaded_weight)
             loaded_params.add(name)
         return loaded_params
 
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index edc2e235c..921d4bc75 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -60,6 +60,8 @@ class AsyncLLM(EngineClient):
         client_count: int = 1,
         client_index: int = 0,
     ) -> None:
+        import blitz_lib
+        blitz_lib.pull_model(vllm_config.model_config.model, vllm_config.parallel_config.tensor_parallel_size * vllm_config.parallel_config.pipeline_parallel_size, vllm_config.parallel_config.tensor_parallel_size, vllm_config.parallel_config.pipeline_parallel_size)
         """
         Create an AsyncLLM.
 
diff --git a/vllm/worker/worker_base.py b/vllm/worker/worker_base.py
index f1c9a0ab0..f52958524 100644
--- a/vllm/worker/worker_base.py
+++ b/vllm/worker/worker_base.py
@@ -511,6 +511,8 @@ class WorkerWrapperBase:
         # one. TODO: investigate if we can remove this field in
         # `WorkerWrapperBase`, `init_cached_hf_modules` should be
         # unnecessary now.
+        import blitz_lib
+        blitz_lib.register_rank(rpc_rank)
         if vllm_config.model_config is not None:
             # it can be None in tests
             trust_remote_code = vllm_config.model_config.trust_remote_code
