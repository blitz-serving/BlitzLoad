diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
index 915f14a29..50dc79246 100644
--- a/vllm/entrypoints/llm.py
+++ b/vllm/entrypoints/llm.py
@@ -2,6 +2,8 @@
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
 import itertools
+import blitz_lib
+
 from collections.abc import Sequence
 from contextlib import contextmanager
 from typing import (TYPE_CHECKING, Any, Callable, ClassVar, Optional, Union,
@@ -276,7 +278,7 @@ class LLM:
         )
 
         log_non_default_args(engine_args)
-
+        blitz_lib.pull_model(model, tensor_parallel_size * 1, tensor_parallel_size, 1)
         # Create the Engine (autoselects V0 vs V1)
         self.llm_engine = LLMEngine.from_engine_args(
             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)
diff --git a/vllm/model_executor/layers/linear.py b/vllm/model_executor/layers/linear.py
index 75391c51f..212bb6134 100644
--- a/vllm/model_executor/layers/linear.py
+++ b/vllm/model_executor/layers/linear.py
@@ -2,6 +2,7 @@
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
 import itertools
+from blitz_lib import vllm_hook
 from abc import abstractmethod
 from typing import Any, Literal, Optional, Union
 
@@ -333,6 +334,7 @@ class ReplicatedLinear(LinearBase):
         else:
             self.register_parameter("bias", None)
 
+    @vllm_hook
     def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
         # If the weight on disk does not have a shape, give it one
         # (such scales for AutoFp8).
@@ -528,6 +530,7 @@ class ColumnParallelLinear(LinearBase):
 
         self.tp_rank = get_tensor_model_parallel_rank()
 
+    @vllm_hook
     def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
 
         output_dim = getattr(param, "output_dim", None)
@@ -656,6 +659,7 @@ class MergedColumnParallelLinear(ColumnParallelLinear):
                          prefix=prefix,
                          return_bias=return_bias)
 
+    @vllm_hook
     def weight_loader(self,
                       param: Parameter,
                       loaded_weight: torch.Tensor,
@@ -1045,6 +1049,7 @@ class QKVParallelLinear(ColumnParallelLinear):
                               shard_offset=shard_offset,
                               shard_size=shard_size)
 
+    @vllm_hook
     def weight_loader(self,
                       param: Parameter,
                       loaded_weight: torch.Tensor,
@@ -1317,6 +1322,7 @@ class RowParallelLinear(LinearBase):
         else:
             self.register_parameter("bias", None)
 
+    @vllm_hook
     def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
         input_dim = getattr(param, "input_dim", None)
         use_bitsandbytes_4bit = getattr(param, "use_bitsandbytes_4bit", False)
diff --git a/vllm/model_executor/layers/vocab_parallel_embedding.py b/vllm/model_executor/layers/vocab_parallel_embedding.py
index 9f223998e..3b483b51f 100644
--- a/vllm/model_executor/layers/vocab_parallel_embedding.py
+++ b/vllm/model_executor/layers/vocab_parallel_embedding.py
@@ -211,6 +211,7 @@ class VocabParallelEmbedding(CustomOp):
         super().__init__()
 
         # Keep the input dimensions.
+        self.prefix = prefix
         tp_rank = get_tensor_model_parallel_rank()
         self.tp_size = get_tensor_model_parallel_world_size()
         self.num_embeddings = num_embeddings
@@ -368,8 +369,9 @@ class VocabParallelEmbedding(CustomOp):
         # If parameter does not have output dim, then it should
         # be copied onto all gpus (e.g. g_idx for act_order gptq).
         if output_dim is None:
-            assert param.data.shape == loaded_weight.shape
-            param.data.copy_(loaded_weight)
+            import blitz_lib
+            print("Loading tensor Embedding")
+            blitz_lib.load_tensor(param, self.prefix)
             return
 
         # Shard indexes for loading the weight
@@ -386,12 +388,16 @@ class VocabParallelEmbedding(CustomOp):
             start_idx = start_idx // packed_factor
             shard_size = shard_size // packed_factor
         else:
-            assert loaded_weight.shape[output_dim] == self.org_vocab_size
+            pass
+            # assert loaded_weight.shape[output_dim] == self.org_vocab_size
 
         # Copy the data. Select chunk corresponding to current shard.
-        loaded_weight = loaded_weight.narrow(output_dim, start_idx, shard_size)
-        param[:loaded_weight.shape[0]].data.copy_(loaded_weight)
-        param[loaded_weight.shape[0]:].data.fill_(0)
+        import blitz_lib
+        print("Tensor embedding")
+        blitz_lib.load_tensor(param, self.prefix)
+        # loaded_weight = loaded_weight.narrow(output_dim, start_idx, shard_size)
+        # param[:loaded_weight.shape[0]].data.copy_(loaded_weight)
+        # param[loaded_weight.shape[0]:].data.fill_(0)
 
     def forward(self, input_):
         if self.tp_size > 1:
diff --git a/vllm/model_executor/model_loader/default_loader.py b/vllm/model_executor/model_loader/default_loader.py
index 2b8e44275..63ad64624 100644
--- a/vllm/model_executor/model_loader/default_loader.py
+++ b/vllm/model_executor/model_loader/default_loader.py
@@ -20,7 +20,7 @@ from vllm.model_executor.model_loader.weight_utils import (
     download_safetensors_index_file_from_hf, download_weights_from_hf,
     fastsafetensors_weights_iterator, filter_duplicate_safetensors_files,
     filter_files_not_needed_for_inference, get_lock, np_cache_weights_iterator,
-    pt_weights_iterator, safetensors_weights_iterator)
+    pt_weights_iterator, safetensors_weights_iterator, dangertensors_weights_iterator)
 from vllm.platforms import current_platform
 
 logger = init_logger(__name__)
@@ -175,9 +175,10 @@ class DefaultModelLoader(BaseModelLoader):
             self, source: "Source"
     ) -> Generator[tuple[str, torch.Tensor], None, None]:
         """Get an iterator for the model weights based on the load format."""
-        hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
-            source.model_or_path, source.revision, source.fall_back_to_pt,
-            source.allow_patterns_overrides)
+        # hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
+        #     source.model_or_path, source.revision, source.fall_back_to_pt,
+        #     source.allow_patterns_overrides)
+        hf_folder, hf_weights_files, use_safetensors, use_dangetensors = ("", [], False, True)
         if self.load_config.load_format == "npcache":
             # Currently np_cache only support *.bin checkpoints
             assert use_safetensors is False
@@ -199,6 +200,12 @@ class DefaultModelLoader(BaseModelLoader):
                     hf_weights_files,
                     self.load_config.use_tqdm_on_load,
                 )
+        elif use_dangetensors:
+            import blitz_lib
+            pull_done = False
+            while not pull_done:
+                pull_done = blitz_lib.check_model(source.model_or_path)
+            weights_iterator = dangertensors_weights_iterator([source.model_or_path], self.load_config.use_tqdm_on_load)
         else:
             weights_iterator = pt_weights_iterator(
                 hf_weights_files,
@@ -221,7 +228,9 @@ class DefaultModelLoader(BaseModelLoader):
         if self.counter_before_loading_weights == 0.0:
             self.counter_before_loading_weights = time.perf_counter()
         # Apply the prefix.
-        return ((source.prefix + name, tensor)
+        # return ((source.prefix + name, tensor)
+        #         for (name, tensor) in weights_iterator)
+        return ((name, tensor)
                 for (name, tensor) in weights_iterator)
 
     def get_all_weights(
@@ -259,6 +268,8 @@ class DefaultModelLoader(BaseModelLoader):
         loaded_weights = model.load_weights(
             self.get_all_weights(model_config, model))
         self.counter_after_loading_weights = time.perf_counter()
+        import blitz_lib
+        blitz_lib.reset_status()
         logger.info(
             "Loading weights took %.2f seconds",
             self.counter_after_loading_weights -
diff --git a/vllm/model_executor/model_loader/weight_utils.py b/vllm/model_executor/model_loader/weight_utils.py
index 78b186265..7a83b0fdd 100644
--- a/vllm/model_executor/model_loader/weight_utils.py
+++ b/vllm/model_executor/model_loader/weight_utils.py
@@ -29,6 +29,10 @@ from vllm.model_executor.layers.quantization import (QuantizationConfig,
 from vllm.platforms import current_platform
 from vllm.utils import PlaceholderModule
 
+from vllm.logger import init_logger
+
+logger = init_logger(__name__)
+
 try:
     from runai_model_streamer import SafetensorsStreamer
 except (ImportError, OSError):
@@ -60,6 +64,7 @@ logger = init_logger(__name__)
 # system reboots, so users will not complain about annoying lock files
 temp_dir = tempfile.gettempdir()
 
+alltime = 0
 
 def enable_hf_transfer():
     """automatically activates hf_transfer
@@ -476,6 +481,23 @@ def safetensors_weights_iterator(
                 yield name, param
 
 
+def dangertensors_weights_iterator(
+    model_name_or_paths: list[str],
+    use_tqdm_on_load: bool,
+) -> Generator[tuple[str, torch.Tensor], None, None]:
+    """Iterate over the weights in the model safetensor files."""
+    import blitz_lib
+    for dt_file in tqdm(
+        model_name_or_paths,
+        desc="Loading safetensors checkpoint shards",
+        disable=not enable_tqdm(use_tqdm_on_load),
+        bar_format=_BAR_FORMAT,
+    ):
+        tensor_metas = blitz_lib.load_tensor_meta(dt_file)
+        for tensor_meta in tensor_metas:
+            name = tensor_meta["name"]
+            yield name, torch.zeros(1)
+
 def runai_safetensors_weights_iterator(
     hf_weights_files: list[str],
     use_tqdm_on_load: bool,
@@ -611,21 +633,37 @@ def convert_pyslice_to_tensor(x: Any) -> torch.Tensor:
     return x
 
 
-def default_weight_loader(param: torch.Tensor,
-                          loaded_weight: torch.Tensor) -> None:
-    """Default weight loader."""
+# def default_weight_loader(param: torch.Tensor,
+#                           loaded_weight: torch.Tensor) -> None:
+#     """Default weight loader."""
+#     try:
+#         if param.numel() == 1 and loaded_weight.numel() == 1:
+#             # Sometimes scalar values aren't considered tensors with shapes
+#             # so if both param and loaded_weight are a scalar,
+#             # "broadcast" instead of copy
+#             param.data.fill_(loaded_weight.item())
+#         else:
+#             assert param.size() == loaded_weight.size(), (
+#                 f"Attempted to load weight ({loaded_weight.size()}) "
+#                 f"into parameter ({param.size()})")
+
+#             param.data.copy_(loaded_weight)
+#     except Exception:
+#         # NOTE: This exception is added for the purpose of setting breakpoint to
+#         # debug weight loading issues.
+#         raise
+
+
+def default_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor, name = None) -> None:
+    """Blitz weight loader."""
     try:
-        if param.numel() == 1 and loaded_weight.numel() == 1:
-            # Sometimes scalar values aren't considered tensors with shapes
-            # so if both param and loaded_weight are a scalar,
-            # "broadcast" instead of copy
-            param.data.fill_(loaded_weight.item())
+        import blitz_lib 
+        # if name:
+        #     print(name, param.device, param.dtype, param.shape)
+        if name: 
+            blitz_lib.load_tensor(param, name)
         else:
-            assert param.size() == loaded_weight.size(), (
-                f"Attempted to load weight ({loaded_weight.size()}) "
-                f"into parameter ({param.size()})")
-
-            param.data.copy_(loaded_weight)
+            blitz_lib.load_tensor(param, "")
     except Exception:
         # NOTE: This exception is added for the purpose of setting breakpoint to
         # debug weight loading issues.
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index edc2e235c..921d4bc75 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -60,6 +60,8 @@ class AsyncLLM(EngineClient):
         client_count: int = 1,
         client_index: int = 0,
     ) -> None:
+        import blitz_lib
+        blitz_lib.pull_model(vllm_config.model_config.model, vllm_config.parallel_config.tensor_parallel_size * vllm_config.parallel_config.pipeline_parallel_size, vllm_config.parallel_config.tensor_parallel_size, vllm_config.parallel_config.pipeline_parallel_size)
         """
         Create an AsyncLLM.
 
diff --git a/vllm/worker/worker_base.py b/vllm/worker/worker_base.py
index f1c9a0ab0..f52958524 100644
--- a/vllm/worker/worker_base.py
+++ b/vllm/worker/worker_base.py
@@ -511,6 +511,8 @@ class WorkerWrapperBase:
         # one. TODO: investigate if we can remove this field in
         # `WorkerWrapperBase`, `init_cached_hf_modules` should be
         # unnecessary now.
+        import blitz_lib
+        blitz_lib.register_rank(rpc_rank)
         if vllm_config.model_config is not None:
             # it can be None in tests
             trust_remote_code = vllm_config.model_config.trust_remote_code
